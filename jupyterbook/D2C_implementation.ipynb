{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D2C Class\n",
    "\n",
    "The `D2C` class is responsible for Dependency to Causality (D2C) analysis. D2C is a method for inferring causal relationships from observational data using simulated directed acyclic graphs (DAGs). The D2C class uses these DAGs to compute asymmetric descriptors from the observations associated with each DAG. This class allows for a comprehensive analysis, offering functions for initialization, computation of descriptors, and model evaluation.\n",
    "\n",
    "## Class Attributes\n",
    "\n",
    "- `DAGs_index`: A numpy array containing the indices of the simulated directed acyclic graphs (DAGs).\n",
    "\n",
    "- `DAGs`: The list of simulated DAGs.\n",
    "\n",
    "- `observations_from_DAGs`: The list of observations associated with each simulated DAG.\n",
    "\n",
    "- `rev`: A boolean flag indicating whether to consider reverse edges. By default, it's set to True, indicating that reverse edges should be considered.\n",
    "\n",
    "- `X`: A DataFrame containing the computed descriptors from the observations. Initially, it's set to None and gets populated by the `initialize` method.\n",
    "\n",
    "- `Y`: A DataFrame containing the labels indicating if an edge is \"is.child\". It's initially set to None and is populated by the `initialize` method.\n",
    "\n",
    "- `verbose`: A boolean flag indicating the verbosity of the execution. If it's set to True, the class will print more information during execution.\n",
    "\n",
    "- `n_jobs`: The number of parallel jobs to run during computation. By default, it's set to 1, indicating no parallelization.\n",
    "\n",
    "- `random_state`: The seed for the random number generator. By default, it's set to 42.\n",
    "\n",
    "\n",
    "## Class Structure\n",
    "\n",
    "- `__init__(self, simulatedDAGs: SimulatedDAGs, rev: bool = True, verbose=False, random_state: int = 42, n_jobs: int = 1) -> None`: \n",
    "The initializer method for the class. It accepts an instance of the SimulatedDAGs class, flags for considering reverse edges, verbosity, a seed for the random state, and the number of jobs for parallel processing.\n",
    "\n",
    "- `initialize(self) -> None`: \n",
    "This method initializes the D2C object by computing descriptors in parallel for all observations.\n",
    "\n",
    "- `_compute_descriptors_for_edge_pairs(self, DAG_index: Any) -> Tuple[list, list]`: \n",
    "A helper method that computes descriptors in parallel for a given observation.\n",
    "\n",
    "- `_generate_edge_pairs(self, DAG_index, dependency_type: str) -> list`: \n",
    "This helper method generates pairs of edges in a DAG according to a specified dependency type.\n",
    "\n",
    "- `_compute_descriptors(self, DAG_index, ca, ef, ns=None, maxs=20, lin=False, acc=True, struct=True, pq= [0.05,0.1,0.25,0.5,0.75,0.9,0.95], boot=\"mrmr\")`: \n",
    "This is a comprehensive function for computing the descriptor of two variables in a dataset under the assumption that one variable is the cause and the other the effect. It includes numerous optional parameters for customization of the descriptor computation process.\n",
    "\n",
    "- `get_df(self) -> pd.DataFrame`: \n",
    "A method to get the concatenated DataFrame of X (descriptors) and Y (labels indicating if an edge is \"is.child\").\n",
    "\n",
    "- `get_score(self, model: RandomForestClassifier = RandomForestClassifier(), test_size: float = 0.2, metric: str = \"accuracy\") -> Union[float, None]`: \n",
    "This method is used to evaluate the performance of a machine learning model using the specified metric. By default, it uses a RandomForestClassifier model and the 'accuracy' metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from multiprocessing import Pool\n",
    "import networkx as nx\n",
    "\n",
    "from typing import Union, Tuple, Any\n",
    "\n",
    "from scipy.stats import skew, tstd\n",
    "from numpy.linalg import inv, pinv\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from d2c.simulatedDAGs import SimulatedDAGs\n",
    "from d2c.utils import *\n",
    "\n",
    "\n",
    "class D2C:\n",
    "    def __init__(self, simulatedDAGs: SimulatedDAGs, rev: bool = True, verbose=False, random_state: int = 42, n_jobs: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        Class for D2C analysis.\n",
    "\n",
    "        D2C (Dependency to Causalilty) analysis is a method for inferring causal relationships\n",
    "        from observational data using simulated directed acyclic graphs (DAGs) and computing \n",
    "        asymmetric descriptors from the observations associated with each DAG.\n",
    "\n",
    "        Args:\n",
    "            simulatedDAGs (SimulatedDAGs): An instance of the SimulatedDAGs class.\n",
    "            rev (bool, optional): Whether to consider reverse edges. Defaults to True.\n",
    "            n_jobs (int, optional): Number of parallel jobs. Defaults to 1.\n",
    "            random_state (int, optional): Random seed. Defaults to 42.\n",
    "        \"\"\"\n",
    "        self.DAGs_index = np.arange(len(simulatedDAGs.list_DAGs))\n",
    "        self.DAGs = simulatedDAGs.list_DAGs\n",
    "        self.observations_from_DAGs = simulatedDAGs.list_observations\n",
    "        self.rev = rev\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        self.verbose = verbose\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def initialize(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the D2C object by computing descriptors in parallel for all observations.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.n_jobs == 1:\n",
    "            results = [self._compute_descriptors_for_edge_pairs(DAG_index) for DAG_index in self.DAGs_index]\n",
    "        else:\n",
    "            with Pool(processes=self.n_jobs) as pool:\n",
    "                results = pool.starmap(\n",
    "                    self._compute_descriptors_for_edge_pairs,\n",
    "                    zip(self.DAGs_index)\n",
    "                )\n",
    "\n",
    "        X_list, Y_list = zip(*results)\n",
    "        self.X = pd.concat([pd.DataFrame(X) for X in X_list], axis=0)\n",
    "        self.Y = pd.concat([pd.DataFrame(Y) for Y in Y_list], axis=0)\n",
    "\n",
    "    def _compute_descriptors_for_edge_pairs(self, DAG_index: Any) -> Tuple[list, list]:\n",
    "        \"\"\"\n",
    "        Compute descriptors in parallel for a given observation.\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        Y = []\n",
    "\n",
    "        edge_pairs = self._generate_edge_pairs(DAG_index,\"is.child\")\n",
    "\n",
    "        for edge_pair in edge_pairs:\n",
    "            parent, child = edge_pair[0], edge_pair[1]\n",
    "            descriptor = self._compute_descriptors(DAG_index, parent, child)\n",
    "            X.append(descriptor)\n",
    "            Y.append(1)  # Label edge as \"is.child\"\n",
    "\n",
    "            if self.rev:\n",
    "                # Reverse edge direction\n",
    "                descriptor_reverse = self._compute_descriptors(DAG_index, child, parent)\n",
    "                X.append(descriptor_reverse)\n",
    "                Y.append(0)  # Label reverse edge as NOT \"is.child\"\n",
    "\n",
    "        return X, Y\n",
    "    \n",
    "    def load_descriptors(self, filename='dataframe.csv'):\n",
    "        descriptors = pd.read_csv(filename)\n",
    "        self.X = descriptors.iloc[:,:-1]\n",
    "        self.Y = descriptors.iloc[:,-1]\n",
    "        #TODO: handle multivariate case\n",
    "\n",
    "\n",
    "    def _generate_edge_pairs(self, DAG_index, dependency_type: str) -> list:\n",
    "        \"\"\"\n",
    "        Generate pairs of edges based on the dependency type.\n",
    "\n",
    "        Args:\n",
    "            dependency_type (str): The type of dependency.\n",
    "\n",
    "        Returns:\n",
    "            list: List of edge pairs.\n",
    "\n",
    "        \"\"\"\n",
    "        edge_pairs = []\n",
    "        if dependency_type == \"is.child\":\n",
    "            for parent_node, child_node in self.DAGs[DAG_index].edges:\n",
    "                edge_pairs.append((parent_node, child_node))\n",
    "        print(\"Edge pairs for DAG\", DAG_index, \"computed:\", edge_pairs)\n",
    "        return edge_pairs\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def _compute_descriptors(self, DAG_index, ca, ef, ns=None, maxs=20,\n",
    "            lin=False, acc=True, struct=True,\n",
    "            pq= [0.05,0.1,0.25,0.5,0.75,0.9,0.95], boot=\"mrmr\"):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute descriptor of two variables in a dataset under the assumption that one variable is the cause and the other the effect.\n",
    "\n",
    "        Parameters:\n",
    "        ca (int): Node index of the putative cause. Must be in the range [0, n).\n",
    "        ef (int): Node index of the putative effect. Must be in the range [0, n).\n",
    "        ns (int, optional): Size of the Markov Blanket. Defaults to min(4, n - 2).\n",
    "        lin (bool, optional): If True, uses a linear model to assess dependency. Defaults to False.\n",
    "        acc (bool, optional): If True, uses the accuracy of the regression as a descriptor. Defaults to True.\n",
    "        struct (bool, optional): If True, uses the ranking in the Markov blanket as a descriptor. Defaults to False.\n",
    "        pq (list of float, optional): A list of quantiles used to compute the descriptor. Defaults to [0.1,0.25,0.5,0.75,0.9].\n",
    "        maxs (int, optional): Max number of pairs MB(i), MB(j) considered. Defaults to 10.\n",
    "        boot (str, optional): Feature selection algorithm. Defaults to \"mimr\".\n",
    "        errd (bool, optional): If True, includes the descriptors of the error. Defaults to False.\n",
    "        delta (bool, optional): Not used in current implementation. Defaults to False.\n",
    "        stabD (bool, optional): Not used in current implementation. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "        dict: A dictionary with the computed descriptors.\n",
    "\n",
    "        Raises:\n",
    "        ValueError: If there are missing or infinite values in D.\n",
    "        \"\"\"\n",
    "        D = self.observations_from_DAGs[DAG_index]\n",
    "\n",
    "        print('Computing descriptors for edge pair: ', ca, ef, 'in DAG', DAG_index)\n",
    "\n",
    "        #scale using pandas\n",
    "        D = (D - D.mean()) / D.std()\n",
    "\n",
    "        if np.any(np.isnan(D)) or np.any(np.isinf(D)): raise ValueError(\"Error: NA or Inf in descriptor\") # Check if there are any missing or infinite values in the data\n",
    "    \n",
    "        # Number of variables\n",
    "        n = D.shape[1]\n",
    "        # Number of observations\n",
    "        N = D.shape[0]\n",
    "\n",
    "\n",
    "        # Set default value for ns if not provided\n",
    "        if ns is None:\n",
    "            ns = min(4, n-2)\n",
    "\n",
    "        # Check that ca and ef are within the valid range\n",
    "        if ca >= n or ef >= n:\n",
    "            raise ValueError(f\"ca={ca}, ef={ef}, n={n}\\nerror in D2C_n\")\n",
    "    \n",
    "\n",
    "        # Initial sets for Markov Blanket\n",
    "        MBca = set(np.arange(n)) - {ca}\n",
    "        MBef = set(np.arange(n)) - {ef}\n",
    "        # intersection of the Markov Blanket of ca and ef\n",
    "        common_causes = MBca.intersection(MBef)\n",
    "        if self.verbose: \n",
    "            print(\"common_causes: \", common_causes)\n",
    "        # Creation of the Markov Blanket of ca (denoted MBca) and ef (MBef)\n",
    "        if n > (ns+1):\n",
    "\n",
    "            if self.verbose: print(\"Computing Markov Blanket\")\n",
    "            # MBca\n",
    "            ind = list(set(np.arange(n)) - {ca})\n",
    "\n",
    "            if self.verbose: \n",
    "                print(\"About to rankrho\")\n",
    "\n",
    "            ind = rankrho(D.iloc[:,ind],D.iloc[:,ca],nmax=min(len(ind),5*ns),verbose=self.verbose) - 1 #python starts from 0\n",
    "            if self.verbose: print('Ind:',ind)\n",
    "            if self.verbose: print('Exited rankrho')\n",
    "\n",
    "            if boot == \"mrmr\":\n",
    "                mrmr = mRMR(D.iloc[:,ind],D.iloc[:,ca],nmax=ns,verbose=self.verbose)\n",
    "                MBca = ind[mrmr]  \n",
    "                if self.verbose: print(\"MBca: \", MBca)\n",
    "            # MBef\n",
    "            ind2 = list(set(np.arange(n)) - {ef})\n",
    "            ind2 = rankrho(D.iloc[:,ind2],D.iloc[:,ef],nmax=min(len(ind2),5*ns),verbose=self.verbose)  \n",
    "            if boot == \"mrmr\":\n",
    "                MBef = ind2[mRMR(D.iloc[:,ind2],D.iloc[:,ef],nmax=ns,verbose=self.verbose)]  \n",
    "                if self.verbose: print(\"MBef: \", MBef)\n",
    "    \n",
    "        if acc:\n",
    "            comcau = 1\n",
    "\n",
    "            if len(common_causes) > 0:\n",
    "                if self.verbose: print(\"common_causes: \", common_causes)\n",
    "                comcau = normalized_conditional_information(D.iloc[:, ef], D.iloc[:, ca], D.iloc[:, list(common_causes)], lin=lin,verbose=self.verbose) \n",
    "\n",
    "            effca = coeff(D.iloc[:, ef], D.iloc[:, ca], D.iloc[:, MBef], verbose=self.verbose)\n",
    "            effef = coeff(D.iloc[:, ca], D.iloc[:, ef], D.iloc[:, MBca], verbose=self.verbose)\n",
    "\n",
    "            if self.verbose: print(\"effca: \", effca, \"effef: \", effef)\n",
    "\n",
    "            ca_ef = normalized_conditional_information(D.iloc[:, ca], D.iloc[:, ef], lin=lin,verbose=self.verbose) \n",
    "            ef_ca = normalized_conditional_information(D.iloc[:, ef], D.iloc[:, ca], lin=lin,verbose=self.verbose) \n",
    "\n",
    "            if self.verbose: print(\"ca_ef: \", ca_ef, \"ef_ca: \", ef_ca)\n",
    "\n",
    "            delta = normalized_conditional_information(D.iloc[:, ef], D.iloc[:, ca], D.iloc[:, MBef], lin=lin,verbose=self.verbose) \n",
    "            delta2 = normalized_conditional_information(D.iloc[:, ca], D.iloc[:, ef], D.iloc[:, MBca], lin=lin,verbose=self.verbose) \n",
    "\n",
    "            if self.verbose: print(\"delta: \", delta, \"delta2: \", delta2)    \n",
    "\n",
    "\n",
    "            delta_i = []  \n",
    "            delta2_i = []\n",
    "            arrays_m_plus_MBca = [np.unique(array).tolist() for array in [np.concatenate(([m], MBca)) for m in MBef]]\n",
    "            arrays_m_plus_MBef = [np.unique(array).tolist() for array in [np.concatenate(([m], MBef)) for m in MBca]]\n",
    "\n",
    "            for array in arrays_m_plus_MBca:\n",
    "                delta_i.append( normalized_conditional_information(D.iloc[:, ef], D.iloc[:, ca], D.iloc[:,  array], lin=lin,verbose=self.verbose))\n",
    "            for array in arrays_m_plus_MBef:\n",
    "                delta2_i.append(normalized_conditional_information(D.iloc[:, ca], D.iloc[:, ef], D.iloc[:,  array], lin=lin,verbose=self.verbose))\n",
    "\n",
    "            if self.verbose: print(\"delta_i: \", delta_i, \"delta2_i: \", delta2_i)\n",
    "\n",
    "            I1_i = [normalized_conditional_information(D.iloc[:, MBef[j]], D.iloc[:, ca], lin=lin,verbose=self.verbose) for j in range(len(MBef))]\n",
    "            I1_j = [normalized_conditional_information(D.iloc[:, MBca[j]], D.iloc[:, ef], lin=lin,verbose=self.verbose) for j in range(len(MBca))]\n",
    "\n",
    "            if self.verbose: print(\"I1_i: \", I1_i, \"I1_j: \", I1_j)\n",
    "\n",
    "            I2_i = [normalized_conditional_information(D.iloc[:, ca], D.iloc[:, MBef[j]], D.iloc[:, ef], lin=lin,verbose=self.verbose) for j in range(len(MBef))]\n",
    "            I2_j = [normalized_conditional_information(D.iloc[:, ef], D.iloc[:, MBca[j]], D.iloc[:, ca], lin=lin,verbose=self.verbose) for j in range(len(MBca))]\n",
    "\n",
    "            if self.verbose: print(\"I2_i: \", I2_i, \"I2_j: \", I2_j)\n",
    "\n",
    "            # Randomly select maxs pairs\n",
    "            IJ = np.array(np.meshgrid(np.arange(len(MBca)), np.arange(len(MBef)))).T.reshape(-1,2)\n",
    "            np.random.shuffle(IJ)\n",
    "            IJ = IJ[:min(maxs, len(IJ))]\n",
    "\n",
    "            if self.verbose: print(\"IJ: \", IJ)\n",
    "\n",
    "            I3_i = [normalized_conditional_information(D.iloc[:, MBca[i]], D.iloc[:, MBef[j]], D.iloc[:, ca], lin=lin,verbose=self.verbose) for i, j in IJ]\n",
    "            I3_j = [normalized_conditional_information(D.iloc[:, MBca[i]], D.iloc[:, MBef[j]], D.iloc[:, ef], lin=lin,verbose=self.verbose) for i, j in IJ]\n",
    "\n",
    "            if self.verbose: print(\"I3_i: \", I3_i, \"I3_j: \", I3_j)\n",
    "\n",
    "            IJ = np.array([(i, j) for i in range(len(MBca)) for j in range(i+1, len(MBca))])\n",
    "            np.random.shuffle(IJ)\n",
    "            IJ = IJ[:min(maxs, len(IJ))]\n",
    "\n",
    "            Int3_i = [normalized_conditional_information(D.iloc[:, MBca[i]], D.iloc[:, MBca[j]], D.iloc[:, ca], lin=lin,verbose=self.verbose) - normalized_conditional_information(D.iloc[:, MBca[i]], D.iloc[:, MBca[j]], lin=lin,verbose=self.verbose) for i, j in IJ]\n",
    "\n",
    "            if self.verbose: print(\"Int3_i: \", Int3_i)\n",
    "\n",
    "            IJ = np.array([(i, j) for i in range(len(MBef)) for j in range(i+1, len(MBef))])\n",
    "            np.random.shuffle(IJ)\n",
    "            IJ = IJ[:min(maxs, len(IJ))]\n",
    "\n",
    "            Int3_j = [normalized_conditional_information(D.iloc[:, MBef[i]], D.iloc[:, MBef[j]], D.iloc[:, ef], lin=lin,verbose=self.verbose) - normalized_conditional_information(D.iloc[:, MBef[i]], D.iloc[:, MBef[j]], lin=lin,verbose=self.verbose) for i, j in IJ]\n",
    "\n",
    "            if self.verbose: print(\"Int3_j: \", Int3_j)\n",
    "\n",
    "            E_ef = ecdf(D.iloc[:, ef],verbose=self.verbose)(D.iloc[:, ef]) \n",
    "            E_ca = ecdf(D.iloc[:, ca],verbose=self.verbose)(D.iloc[:, ca])\n",
    "\n",
    "            if self.verbose: print(\"E_ef: \", E_ef, \"E_ca: \", E_ca)\n",
    "\n",
    "            gini_ca_ef = normalized_conditional_information(D.iloc[:, ca], pd.DataFrame(E_ef), lin=lin,verbose=self.verbose)\n",
    "            gini_ef_ca = normalized_conditional_information(D.iloc[:, ef], pd.DataFrame(E_ca), lin=lin,verbose=self.verbose)\n",
    "\n",
    "            if self.verbose: print(\"gini_ca_ef: \", gini_ca_ef, \"gini_ef_ca: \", gini_ef_ca)\n",
    "\n",
    "            gini_delta = normalized_conditional_information(D.iloc[:, ef], pd.DataFrame(E_ca), D.iloc[:, MBef], lin=lin,verbose=self.verbose)\n",
    "            gini_delta2 = normalized_conditional_information(D.iloc[:, ca], pd.DataFrame(E_ef), D.iloc[:, MBca], lin=lin,verbose=self.verbose)\n",
    "\n",
    "            if self.verbose: print(\"gini_delta: \", gini_delta, \"gini_delta2: \", gini_delta2)\n",
    "\n",
    "            namesx = [\"effca\",\"effef\",\"comcau\",\"delta\",\"delta2\"]\n",
    "            namesx += [\"delta.i\" + str(i+1) for i in range(len(pq))]\n",
    "            namesx += [\"delta2.i\" + str(i+1) for i in range(len(pq))]\n",
    "            namesx += [\"ca.ef\",\"ef.ca\"]\n",
    "            namesx += [\"I1.i\" + str(i+1) for i in range(len(pq))]\n",
    "            namesx += [\"I1.j\" + str(i+1) for i in range(len(pq))]\n",
    "            namesx += [\"I2.i\" + str(i+1) for i in range(len(pq))]\n",
    "            namesx += [\"I2.j\" + str(i+1) for i in range(len(pq))]\n",
    "            namesx += [\"I3.i\" + str(i+1) for i in range(len(pq))]\n",
    "            namesx += [\"I3.j\" + str(i+1) for i in range(len(pq))]\n",
    "            namesx += [\"Int3.i\" + str(i+1) for i in range(len(pq))]\n",
    "            namesx += [\"Int3.j\" + str(i+1) for i in range(len(pq))]\n",
    "            namesx += [\"gini.delta\",\"gini.delta2\",\"gini.ca.ef\",\"gini.ef.ca\"]\n",
    "\n",
    "            keys = namesx\n",
    "            \n",
    "            values = [effca, effef, comcau, delta, delta2]\n",
    "            values.extend(np.quantile(delta_i, q=pq, axis=0).flatten()) \n",
    "            values.extend(np.quantile(delta2_i, q=pq, axis=0).flatten()) \n",
    "            values.extend([ca_ef, ef_ca])\n",
    "            values.extend(np.quantile(I1_i, q=pq, axis=0).flatten()) \n",
    "            values.extend(np.quantile(I1_j, q=pq, axis=0).flatten()) \n",
    "            values.extend(np.quantile(I2_i, q=pq, axis=0).flatten()) \n",
    "            values.extend(np.quantile(I2_j, q=pq, axis=0).flatten()) \n",
    "            values.extend(np.quantile(I3_i, q=pq, axis=0).flatten()) \n",
    "            values.extend(np.quantile(I3_j, q=pq, axis=0).flatten()) \n",
    "            values.extend(np.quantile(Int3_i, q=pq, axis=0).flatten()) \n",
    "            values.extend(np.quantile(Int3_j, q=pq, axis=0).flatten()) \n",
    "            values.extend([gini_delta, gini_delta2,gini_ca_ef, gini_ef_ca]) \n",
    "\n",
    "            \n",
    "            # Replace NA values with 0\n",
    "            dictionary = dict(zip(keys, values))\n",
    "            # for key in dictionary:\n",
    "            #     if np.isnan(dictionary[key]):\n",
    "            #         dictionary[key] = 0\n",
    "            \n",
    "            print(\"Descriptors for DAG\", DAG_index, \"edge pair\", ca, ef, \"computed\")\n",
    "            return dictionary\n",
    "\n",
    "\n",
    "    def get_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the concatenated DataFrame of X and Y.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The concatenated DataFrame of X and Y.\n",
    "\n",
    "        \"\"\"\n",
    "        return pd.concat([self.X,self.Y], axis=1)\n",
    "    \n",
    "\n",
    "    def get_score(self, model: RandomForestClassifier = RandomForestClassifier(), test_size: float = 0.2, metric: str = \"accuracy\") -> Union[float, None]:\n",
    "        \"\"\"\n",
    "        Get the score of a machine learning model using the specified metric.\n",
    "\n",
    "        Parameters:\n",
    "            model (RandomForestClassifier): The machine learning model to evaluate.\n",
    "            test_size (float): The proportion of the data to use for testing.\n",
    "            metric (str): The evaluation metric to use (default is \"accuracy\"). Valid metrics are: 'accuracy', 'f1', 'precision', 'recall', 'auc'.\n",
    "\n",
    "        Returns:\n",
    "            float: The score of the model using the specified metric.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If an invalid metric is provided.\n",
    "\n",
    "        \"\"\"\n",
    "        data = self.X\n",
    "        labels = self.Y\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, labels, train_size=1-test_size, test_size=test_size, random_state=self.random_state)\n",
    "\n",
    "        y_train = y_train.values.ravel()\n",
    "        y_test = y_test.values.ravel()\n",
    "\n",
    "        # Create an instance of the Random Forest classifier\n",
    "        model = RandomForestClassifier(n_jobs=self.n_jobs, random_state=self.random_state)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Get the accuracy of the model\n",
    "        if metric == \"accuracy\":\n",
    "            return model.score(X_test, y_test)\n",
    "        elif metric == \"f1\":\n",
    "            return f1_score(y_test, model.predict(X_test))\n",
    "        elif metric == \"precision\":\n",
    "            return precision_score(y_test, model.predict(X_test))\n",
    "        elif metric == \"recall\":\n",
    "            return recall_score(y_test, model.predict(X_test))\n",
    "        elif metric == \"auc\":\n",
    "            return roc_auc_score(y_test, model.predict(X_test))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid metric. Valid metrics are: 'accuracy', 'f1', 'precision', 'recall'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
