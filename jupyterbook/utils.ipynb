{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.linalg import inv, pinv, solve\n",
    "from scipy.special import expit\n",
    "from numpy.random import default_rng\n",
    "from scipy.stats import entropy, zscore, skew, tstd, percentileofscore\n",
    "\n",
    "from sklearn.metrics.pairwise import polynomial_kernel\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ridge_regression` function is a Ridge Regression implementation that trains a model, predicts, and computes metrics. \n",
    "\n",
    "Ridge Regression is a linear regression variant that includes a regularization term, regulated by `lambda_val`, to prevent model overfitting. It accepts four arguments: `X_train`, `Y_train`, `X_test`, and `lambda_val`. `X_train` and `Y_train` are numpy arrays representing the training design matrix and the training response vector respectively, where each row in `X_train` represents an observation and each corresponding element in `Y_train` represents the target value. \n",
    "\n",
    "An optional argument is `X_test`, the test design matrix, used for making predictions with the trained model. Another optional argument is `lambda_val`, a float that defaults to 0.001, regulating the strength of the regularization term in the Ridge Regression formula.\n",
    "\n",
    "The function returns a dictionary containing various elements. The training residuals `e_train` represents the difference between the actual and predicted values. `MSE_emp` denotes the mean squared error on the training data, giving an indication of the model's prediction error. `NMSE`, or Normalized Mean Squared Error, provides a relative measure of the prediction error, allowing comparisons between different models or datasets. `MSE_loo` is the mean squared error computed using leave-one-out cross-validation, a robust way to estimate model performance. \n",
    "\n",
    "The function also returns the predicted values for the training data, `Y_train_hat`, and the test data, `Y_test_hat` (which is None if no test data was provided). Lastly, the trained Ridge Regression model itself is returned as `model`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X_train, Y_train, X_test=None, lambda_val=1e-3):\n",
    "    \"\"\"\n",
    "    Perform ridge regression and returns the trained model, predictions, and metrics.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): The training design matrix.\n",
    "        Y_train (np.ndarray): The training response vector.\n",
    "        X_test (np.ndarray, optional): The test design matrix. Defaults to None.\n",
    "        lambda_val (float, optional): The regularization parameter. Defaults to 1e-3.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the trained model, predictions, and computed metrics.\n",
    "    \"\"\"\n",
    "    model = Ridge(alpha=lambda_val)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    Y_train_hat = model.predict(X_train)\n",
    "    e_train = Y_train - Y_train_hat\n",
    "    MSE_emp = mean_squared_error(Y_train, Y_train_hat)\n",
    "    NMSE = MSE_emp / (np.var(Y_train)**2)\n",
    "\n",
    "    e_loo = cross_val_score(model, X_train, Y_train, scoring='neg_mean_squared_error', cv=len(X_train))\n",
    "    MSE_loo = -np.mean(e_loo)\n",
    "\n",
    "    Y_test_hat = None\n",
    "    if X_test is not None:\n",
    "        Y_test_hat = model.predict(X_test)\n",
    "\n",
    "    return {\n",
    "        'e_train': e_train,\n",
    "        'MSE_emp': MSE_emp,\n",
    "        'NMSE': NMSE,\n",
    "        'MSE_loo': MSE_loo,\n",
    "        'Y_train_hat': Y_train_hat,\n",
    "        'Y_test_hat': Y_test_hat,\n",
    "        'model': model,\n",
    "    }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `column_based_correlation` function computes the correlation between each column in a matrix `X` and a vector `Y`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_based_correlation(X,Y,verbose=True):\n",
    "    #TODO: multidimensional Y \n",
    "    if verbose: print('column_based_correlation')\n",
    "    columns_of_X = X.shape[1]  # Number of columns in X\n",
    "\n",
    "    correlation_vector = np.zeros(columns_of_X)  # Initialize correlation vector\n",
    "\n",
    "    for i in range(columns_of_X):\n",
    "        correlation_matrix = np.corrcoef(X.iloc[:, i], Y.iloc[:, 0])\n",
    "        correlation_value = correlation_matrix[0, 1]\n",
    "        correlation_vector[i] = correlation_value\n",
    "\n",
    "    correlation_array = correlation_vector.reshape(1, -1)\n",
    "\n",
    "    # Print the correlation vector\n",
    "    return(correlation_array[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `co2i` function converts correlations to mutual information, a quantity which measures the dependency between variables. It calls the `column_based_correlation` function to compute correlation between each column in `X` and `Y`. It squares this correlation and then uses it to compute mutual information. If `Y` is a `pandas.Series`, it converts it to a `DataFrame`. The function also supports a `verbose` flag that, when set to `True`, prints the operations being performed and the computed mutual information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co2i(X,Y, verbose=True):\n",
    "\n",
    "    # check if Y is a pd.series and make it dataframe\n",
    "    if isinstance(Y, pd.Series):\n",
    "        Y = pd.DataFrame(Y)\n",
    "\n",
    "    if verbose: print('co2i')\n",
    "\n",
    "    correlation_vector = column_based_correlation(X,Y, verbose=verbose)\n",
    "    corr_sq = np.square(correlation_vector)\n",
    "\n",
    "    I = -0.5 * np.log(1 - corr_sq)\n",
    "    if verbose: print('I: ', I)\n",
    "\n",
    "    return I\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the mutual information between each column of `X` and `Y`, and returns the indices of the columns in `X` that have the highest mutual information with `Y`. The number of indices returned is determined by thenmax parameter.\n",
    "\n",
    "If the variance of `Y` is less than 0.01, the function returns a list of indices ranging from 1 to nmax. If regr is False, the function uses the co2i function to calculate the mutual information. If regr is True, ridge regression is performed for each column of X with Y as the target variable, and the maximum coefficient value is\n",
    "used as the mutual information.\n",
    "\n",
    "The input arrays X and Y are expected to have compatible shapes, where X has shape (N, n) and Y has shape (N,). The function assumes that the columns of X and Y correspond to the same samples.\n",
    "\n",
    "Example:\n",
    "```\n",
    "X =    [[1, 2, 3],\n",
    "        [4, 5, 6],\n",
    "        [7, 8, 9]]\n",
    "\n",
    "Y = [10, 20, 30]\n",
    "\n",
    "top_features = rankrho(X, Y, nmax=2, regr=True)\n",
    "\n",
    "# Output: [3, 2]\n",
    "# The third column of X has the highest mutual information with Y, followed by the second column.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankrho(X, Y, nmax=5, regr=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Perform mutual information ranking between two arrays.\n",
    "\n",
    "    Parameters:\n",
    "        X (array-like): Input array with shape (N, n), representing N samples and n features.\n",
    "        Y (array-like): Input array with shape (N,). Target variable.\n",
    "        nmax (int, optional): Number of top-ranked features to return. Defaults to 5.\n",
    "        regr (bool, optional): Flag indicating whether to use ridge regression for ranking. Defaults to False.\n",
    "        verbose (bool, optional): Flag indicating whether to display progress information. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list: Indices of the top-ranked features in X based on mutual information with Y.\n",
    "\n",
    "\n",
    "       \n",
    "    \"\"\"\n",
    "    if verbose: print('rankrho')\n",
    "    # Number of columns in X and Y\n",
    "    n = X.shape[1]\n",
    "    # m = Y.shape[1] #TODO: handle the multivariate case\n",
    "    N = X.shape[0]\n",
    "\n",
    "    if np.var(Y) < 0.01:\n",
    "        if verbose: print('np.var(Y) < 0.01')\n",
    "        return list(range(1, nmax + 1))\n",
    "    \n",
    "    # Scaling X\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "    Iy = np.zeros(n)\n",
    "\n",
    "    if not regr:\n",
    "        if verbose: print('not regr')\n",
    "        Iy = co2i(X, Y, verbose=verbose)\n",
    "        if verbose: print(Iy)\n",
    "    else:\n",
    "        if verbose: print('regr')\n",
    "        for i in range(n):\n",
    "            Iy[i] = abs(ridge_regression(X[:, i], Y)['beta_hat'][1])\n",
    "\n",
    "    # if m > 1:\n",
    "    #     Iy = np.mean(Iy, axis=1)\n",
    "\n",
    "    return (np.argsort(Iy)[::-1] + 1)[:nmax]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mRMR` function implements the Maximum Relevance Minimum Redundancy (mRMR) feature selection method. It calculates the mutual information between each feature in `X` and `Y`, then iteratively selects the features with high relevance to `Y` and low redundancy with the already selected features. \n",
    "\n",
    "The function starts by selecting the feature with the maximum mutual information with `Y`. Then, for `nmax - 1` iterations, it calculates the mutual information between each remaining feature and the currently selected features, computes the mRMR score for each remaining feature (the difference between its mutual information with `Y` and the average mutual information with the selected features), and selects the feature with the maximum mRMR score.\n",
    "\n",
    "It returns the indices of the selected features in the order they were selected. The function supports a `verbose` flag that, when set to `True`, prints the operations being performed and intermediate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mRMR(X, Y, nmax, verbose=True):\n",
    "\n",
    "    if verbose: print('mRMR')\n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    # Calculate mutual information between each feature in X and Y\n",
    "    mi_XY = mutual_info_regression(X, Y)\n",
    "    if verbose: print(\"mi_XY: \", mi_XY)\n",
    "\n",
    "    # Start with the feature with maximum MI with Y\n",
    "    indices = [np.argmax(mi_XY)]\n",
    "    \n",
    "    for _ in range(nmax - 1):\n",
    "        remaining_indices = list(set(range(num_features)) - set(indices))\n",
    "        if verbose: print(\"remaining_indices: \", remaining_indices)\n",
    "        mi_XX = np.zeros(len(remaining_indices))\n",
    "        \n",
    "        # Calculate mutual information between selected features and remaining features\n",
    "        for i in range(len(remaining_indices)):\n",
    "            mi_XX[i] = mutual_info_regression(X.iloc[:, indices], X.iloc[:, remaining_indices[i]])[0]\n",
    "        \n",
    "        # Calculate MRMR score for each remaining feature\n",
    "        mrmr_scores = mi_XY[remaining_indices] - np.mean(mi_XX)\n",
    "        if verbose: print(\"mrmr_scores: \", mrmr_scores)\n",
    "        # Select feature with maximum MRMR score\n",
    "        indices.append(remaining_indices[np.argmax(mrmr_scores)])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `normalized_prediction` function computes the normalized mean squared error of the dependency, which is a measure of the quality of a predictive model. It performs data preprocessing on `X` to remove constant and `NaN`-containing columns, and normalizes `X` by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "The function begins by ensuring `X` is a DataFrame if it is initially a `pd.Series`. It checks if `X` has more than one feature. If so, it removes constant and `NaN`-containing columns. If `X` only contains one feature or less, it checks for `NaN` values and, if found, it returns 1.\n",
    "\n",
    "Once `X` is processed, it's standardized to have zero mean and unit variance for each feature. The function checks if there are too few samples (`N < 5`) or if `NaN` values exist after normalization, in which case it returns the variance of `Y`.\n",
    "\n",
    "If the `lin` argument is `True`, which is the default, the function calculates the normalized mean squared error using `ridge_regression` function and returns it. In case a nonlinear prediction method is desired, it's flagged with a `TODO` comment indicating it has not been implemented yet.\n",
    "\n",
    "Verbose mode is supported which, when set to `True`, prints the operations being performed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalized_prediction(X, Y, lin=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Normalized mean squared error of the dependency\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose: print('normalized_prediction')\n",
    "    if isinstance(X, pd.Series):\n",
    "        X = pd.DataFrame(X)\n",
    "    if verbose: print(X.shape)    \n",
    "    N, n = X.shape\n",
    "\n",
    "    if n > 1: # TODO: check the case if all columns are constant, return 1\n",
    "        if verbose: print('n > 1')\n",
    "        X = np.delete(X, np.where(np.std(X, axis=0) < 0.01)[0], axis=1) # if there is any constant column, remove it\n",
    "        X = np.delete(X, np.where(np.isnan(np.sum(X, axis=0)))[0], axis=1) # if there is any nan, remove it\n",
    "    else:\n",
    "        if verbose: print('n <= 1')\n",
    "        if np.any(np.isnan(X)): return 1 # TODO: check this\n",
    "            \n",
    "    XX = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "    if N < 5 or np.any(np.isnan(XX)): \n",
    "        if verbose: print('N < 5 or np.any(np.isnan(XX))')\n",
    "        return np.var(Y)\n",
    "    if lin: \n",
    "        if verbose: print('lin')\n",
    "        return max(1e-3, ridge_regression(XX, Y)['MSE_loo'] / (1e-3 + np.var(Y)))\n",
    "\n",
    "    #TODO: implement the nonlinear case\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `normalized_conditional_information` function computes the normalized conditional information between two datasets `x1` and `y`, given a dataset `x2`. This is a measure of the amount of information that `x1` provides about `y` that is not already provided by `x2`. \n",
    "\n",
    "The function checks if `x2` is provided. If not, it computes the normalized prediction of `y` given `x1` (which serves as an estimate of the entropy of `y` given `x1`), and returns the quantity `1 - entropy_y_given_x1` as the normalized information between `x1` and `y`. \n",
    "\n",
    "If `x2` is provided, it computes the normalized prediction of `y` given `x2` and `x1` concatenated with `x2`, and returns the difference between `entropy_y_given_x2` and `entropy_y_given_x1_x2`, normalized by `entropy_y_given_x2`. \n",
    "\n",
    "This method essentially quantifies how much `x1` can further explain `y` even when `x2` is already known. The function supports a `verbose` mode, which, when set to `True`, prints the operations being performed and intermediate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_conditional_information(y, x1, x2=None, lin=True, verbose=True):\n",
    "        \"\"\"\n",
    "        Normalized conditional information of x1 to y given x2\n",
    "        I(x1;y| x2)= (H(y|x2)-H(y | x1,x2))/H(y|x2)\n",
    "        \"\"\"\n",
    "        if verbose: print('normalized_conditional_information')\n",
    "\n",
    "        if x2 is None:  # I(x1;y)= (H(y)-H(y | x1))/H(y)\n",
    "            if verbose: print('x2 is None')\n",
    "            entropy_y_given_x1 = normalized_prediction(x1, y, verbose=verbose)\n",
    "            return max(0, 1 - entropy_y_given_x1)\n",
    "\n",
    "        if verbose: print('x2 is not None')\n",
    "        entropy_y_given_x2 = normalized_prediction(x2, y, verbose=verbose)\n",
    "        x1_x2 = pd.concat([x1, x2],axis=1)\n",
    "        entropy_y_given_x1_x2 = normalized_prediction(x1_x2, y, verbose=verbose)\n",
    "        if verbose: print('entropy_y_given_x2: ', entropy_y_given_x2)\n",
    "        return max(0, entropy_y_given_x2 - entropy_y_given_x1_x2 ) / (entropy_y_given_x2 + 0.01)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
